For Paper Presentation, the presenter should upload the following files to each given folder:

- *Report*: briefly summarize the paper. It should be structured as a memo, including introduction, related work, solution, experimental results, major conclusions, pros and cons.
- *Demo*: an interactive Jupyter Notebook / Colab to show visualize the result of the paper. For most paper, it's highly recommended that you download the source code of the paper, running the main model and see the results or visualization in this notebook.
- *Slides*: which you use to present at the course. It should contain both the contents of the report and some key results of the code demo.
- *Quiz*: 3 questions about this paper. Please send me the question bank one day before the class, and provide answers and feedback.

Every audience should participate in Q&A after the presentation, and finish the quiz using the quiz link after the course.


The schedule for paper presentation is:

- GNNs and Knowledge Graphs (4 lectures)
  - [1/14] Ziniu Hu, Yuxiao Dong, Kuansan Wang, Yizhou Sun, Heterogeneous Graph Transformer. WWW 2020. https://arxiv.org/abs/2003.01332 (**Guest Lecture**)
    - [**(Presentation)**](Knowledge_Graph/Hetergeneous_Graph_Transformer/) Guest Lecturer: **Ziniu Hu (bull@cs.ucla.edu)**
  - [1/19] Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, Partha Talukdar, Composition-based Multi-Relational Graph Convolutional Networks. ICLR 2020. https://arxiv.org/abs/1911.03082
    - [**(Presentation)**](Knowledge_Graph/Composition-based_Multi-Relational_Graph_Convolutional_Networks/) Presenters: **Sripath Mishra (Mishra60@g.ucla.edu)**, **Te-Lin Wu (telinwu@cs.ucla.edu)**
  - [1/21] Yuyu Zhang, Xinshi Chen, Yuan Yang, Arun Ramamurthy, Bo Li, Yuan Qi, Le Song, Efficient Probabilistic Logic Reasoning with Graph Neural Networks. ICLR 2020. https://arxiv.org/abs/2001.11850
    - [**(Presentation)**](Knowledge_Graph/Efficient_Probabilistic_Logic_Reasoning_with_Graph_Neural_Networks/) Presenters: **Jingyue Shen** (brianshen@ucla.edu), **Boyuan He**, **Haochen Li(joshuali1997@yahoo.com)**
  - [1/26] Hongyu Ren, Jure Leskovec, Beta Embeddings for Multi-Hop Logical Reasoning in Knowledge Graphs. NeurIPS 2020. https://arxiv.org/abs/2010.11465
    - [**(Presentation)**](Knowledge_Graph/BetaE/) Presenters: **Zongyue Qin (qinzongyue@cs.ucla.edu)**, **Qian Long**, **Benlin Liu (liubenlin@cs.ucla.edu)**
- GNN and dynamic systems (3 lectures)
  - [1/28] Peter W. Battaglia, Razvan Pascanu, Matthew Lai, Danilo Rezende, Koray Kavukcuoglu, Interaction Networks for Learning about Objects, Relations and Physics, NIPS 2016. https://arxiv.org/abs/1612.00222
    - [**(Presentation)**](Dynamic_Graph/Interaction_Networks/) Presenters: **Shuwen Qiu**, **Jiayue Sun (jysun@cs.ucla.edu)**, **Qing Li (dylan.liqing@gmail.com)**
  - [2/2] Zijie Huang, Yizhou Sun, Wei Wang, Learning Continuous System Dynamics from Irregularly-Sampled Partial Observations, NeurIPS 2020. https://arxiv.org/abs/2011.03880 (**Guest Lecture**)
    - [**(Presentation)**](Dynamic_Graph/LG-ODE/)Guest Lecturer: **Zijie Huang**
  - [2/4] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, Peter W. Battaglia, Learning to Simulate Complex Physics with Graph Networks, ICML 2020. https://arxiv.org/abs/2002.09405
    - [**(Presentation)**](Dynamic_Graph/GNS) Presenters: **Yuanhao Xiong, Xiangning Chen, Li-Cheng Lan**

- GNNs and Programming Language (2 lectures)
  - [2/9] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, Ke Wang, Hoppity: Learning Graph Transformations to Detect and Fix Bugs In Programs, ICLR 2020. https://openreview.net/forum?id=SJeqs6EFvB
    - [**(Presentation)**](Programming_Language/HOPPITY/) Presenters: **Liunian Li (liunian.harold.li@cs.ucla.edu)**, **Yujun Zhao (yujunzhao.ming@gmail.com)**, **Ruochen Wang (ruocwang@ucla.edu)**
  - [2/11] David Bieber, Charles Sutton, Hugo Larochelle, Daniel Tarlow, Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks, NeurIPS 2020. https://arxiv.org/abs/2010.12621
    - [**(Presentation)**](Programming_Language/IPAGNN/) Presenters: **Hemil Desai**, **Zixiang Chen(chenzx19@cs.ucla.edu)**, **Jiafan He(jiafanhe19@ucla.edu)**
- Graph Synthesis (4 lectures)
  - [2/16] Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, Jure Leskovec, GCPN: Graph Convolutional Policy Network, NeurIPS 2018. https://arxiv.org/abs/1806.02473
      - [**(Presentation)**](Graph_Synthesis/GCPN/) Presenters: **Howard Xie (howardx@cs.ucla.edu), Shanxiu He (heshanxiu@g.ucla.edu), Justin Yi (joostinyi00@gmail.com)**
  - [2/18] Nicola De Cao, Thomas Kipf, MolGAN: An implicit generative model for small molecular graphs, 2018. https://arxiv.org/pdf/1805.11973.pdf
      - [**(Presentation)**](Graph_Synthesis/MolGAN/) Presenters: **Daisy Zheng (dayz@g.ucla.edu)**, **Nilay Shah (nshah76@ucla.edu)**, **Nima Zaghari (nzaghari@cs.ucla.edu)**
  - [2/23] Aditya Grover, Aaron Zweig, Stefano Ermon, Graphite: Iterative Generative Modeling of Graphs. ICML 2019. http://proceedings.mlr.press/v97/grover19a.html.
      - [**(Presentation)**](Graph_Synthesis/Graphite/) Presenters: **Tameez Latib (tameezlatib@gmail.com)**, **Mia Levy (miamlevy@yahoo.com)**, **Hengda Shi (hengda.shi@cs.ucla.edu)**
  - [2/25] Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, Jian Tang, GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation. ICLR 2020. https://arxiv.org/abs/2001.09382
      - Presenters: **Daniel Ahn (dahn@g.ucla.edu)** **Xuan Lin (xuanlin1991@gmail.com)**
  - (Optional Reading) You, J., Ying, R., Ren, X., Hamilton, W. L., and Leskovec, J. GraphRNN: A deep generative model for graphs. ICML 2018.
  - (Optional Reading) Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K. Duvenaud, Raquel Urtasun, Richard Zemel, Efficient Graph Generation with Graph Recurrent Attention Networks. NeurIPS 2019.
  - (Optional Reading) Scalable Deep Generative Modeling for Sparse Graphs. ICML 2020. https://arxiv.org/pdf/2006.15502.pdf
- Expressive Power of GNNs (2 lectures)
  - [3/2] (GNN and WL) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks?  ICLR 2019.  
      - [**(Presentation)**](Expressive_Power/PowerfulGNNs/) Presenters: **Yu Yang (yuyang@cs.ucla.edu)**, **Yihe Deng (yihedeng@g.ucla.edu)**, **Mingyu Derek Ma (ma@cs.ucla.edu)**
  - [3/4] (High-Order GNNs) Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, Yaron Lipman, Provably Powerful Graph Networks, NeurIPS, 2019. https://arxiv.org/abs/1905.11136
    - Presenters: **Difan Zou**, **Lucas Tecot** (lucastecot@gmail.edu), **Weitong Zhang(weightzero@g.ucla.edu)**
  - (Optional Reading) Ryoma Sato, Makoto Yamada, and Hisashi Kashima.  Random features strengthen graph neural networks. arXiv preprint, 2020.
  - (Optional Reading) Can Graph Neural Networks Count Substructures. NeurIPS 2020.
  - (Optional Reading) Ryoma Sato, A Survey on The Expressive Power of Graph Neural Networks. https://arxiv.org/abs/2003.04078
